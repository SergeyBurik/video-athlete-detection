{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ниже приведены **два** решения задачи.\n",
        "\n",
        "**Первое** использует модель **Grounding DINO**, которая находит спортсмена по **текстовому описанию**. Модель работает с минимальным количеством ложных детекций и демонстрирует **стабильно точные результаты**, однако каждый вызов этой модели длится дольше, чем более легковесные аналоги вроде YOLO.\n",
        "\n",
        "**Второе** решение является **оптимизированной версией** первого решения. В нем **минимизируется количество вызовов** модели Grounding DINO. Эта модель вызывается всего 2 раза в секунду. Bounding boxes этой модели являются опорой для **более быстрой модели** YOLO (или любой аналогичной модели)."
      ],
      "metadata": {
        "id": "ZgmrzaDyH5l5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот код реализует систему автоматического **обнаружения, отслеживания и оценки позы спортсмена на видео**.\n",
        "\n",
        "Он использует модель **Grounding DINO** для детекции по текстовому описанию с частотой 5 раз в секунду.\n",
        "Затем активируется **трекер CSRT**, поддерживающий отслеживание спортсмена в промежутках **между детекциями**.\n",
        "\n",
        "Для повышения скорости детекция выполняется на **уменьшенной копии кадра**, а координаты масштабируются обратно.\n",
        "\n",
        "Если объект **успешно детектирован** или оттрекан, производится **обрезка** соответствующей **области**, на которую затем применяется модель YOLO **для оценки позы**.\n",
        "\n",
        "Полученные ключевые точки отображаются на оригинальном кадре в **виде красных точек**.\n",
        "\n",
        "Готовое видео записывается **в выходной файл**. Также в коде учитывается возможность работы на GPU с использованием смешанной точности для ускорения вычислений."
      ],
      "metadata": {
        "id": "raCMpDP1Eoy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install deep-sort-realtime\n",
        "!pip install openmim\n",
        "!mim install mmdet\n",
        "!mim install mmpose\n",
        "\n",
        "# Grounding Dino\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "!cd GroundingDINO\n",
        "!pip install --upgrade pip setuptools wheel cython\n",
        "!pip install -r requirements.txt\n",
        "!pip install pycocotools supervision yapf addict\n",
        "!pip install -e . --no-build-isolation\n",
        "\n"
      ],
      "metadata": {
        "id": "aivqb7jI0p-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LK6Zjaf0lUn"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "\n",
        "start_execution_time = time.time()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
        "\n",
        "pose_model = YOLO('yolo11m-pose.pt')\n",
        "\n",
        "# Mixed precision\n",
        "use_amp = device == 'cuda'\n",
        "if use_amp:\n",
        "    model = model.half()\n",
        "\n",
        "cap = cv2.VideoCapture('/content/0LtLS9wROrk_E_000731_000738.mp4')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "w, h = int(cap.get(3)), int(cap.get(4))\n",
        "out = cv2.VideoWriter('gdino_out_1.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "\n",
        "# Parameters\n",
        "DETECTION_FPS = 5  # раз в секунду\n",
        "FRAME_SKIP = int(fps / DETECTION_FPS)\n",
        "SCALE_FACTOR = 1/10\n",
        "prompt = [[\"a gymnast\", \"a gymnast upside down\"]]\n",
        "\n",
        "frame_count = 0\n",
        "last_detection = None\n",
        "tracker = None\n",
        "tracker_active = False\n",
        "\n",
        "small_w, small_h = int(w * SCALE_FACTOR), int(h * SCALE_FACTOR)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    current_detection = None\n",
        "\n",
        "    if frame_count % FRAME_SKIP == 0:\n",
        "        small_frame = cv2.resize(frame, (small_w, small_h))\n",
        "        img = Image.fromarray(cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB))\n",
        "        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n",
        "\n",
        "        if use_amp:\n",
        "            inputs = {k: v.to(device).half() if v.dtype == torch.float32 else v.to(device)\n",
        "                      for k, v in inputs.items()}\n",
        "        else:\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(**inputs)\n",
        "            else:\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "        results = processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs['input_ids'],\n",
        "            box_threshold=0.3,\n",
        "            text_threshold=0.25,\n",
        "            target_sizes=[(small_h, small_w)]\n",
        "        )\n",
        "\n",
        "        boxes = results[0][\"boxes\"]\n",
        "        scores = results[0][\"scores\"]\n",
        "        labels = results[0][\"labels\"]\n",
        "\n",
        "        if len(scores) > 0:\n",
        "            max_score_idx = torch.argmax(scores).item()\n",
        "            best_box = boxes[max_score_idx].tolist()\n",
        "            best_score = scores[max_score_idx].item()\n",
        "            best_label = labels[max_score_idx]\n",
        "\n",
        "            scale_x = w / small_w\n",
        "            scale_y = h / small_h\n",
        "            best_box = [\n",
        "                int(best_box[0] * scale_x),\n",
        "                int(best_box[1] * scale_y),\n",
        "                int(best_box[2] * scale_x),\n",
        "                int(best_box[3] * scale_y)\n",
        "            ]\n",
        "\n",
        "            current_detection = {\n",
        "                'box': best_box,\n",
        "                'score': best_score,\n",
        "                'label': best_label\n",
        "            }\n",
        "            last_detection = current_detection\n",
        "\n",
        "            tracker = cv2.TrackerCSRT_create()\n",
        "            x1, y1, x2, y2 = best_box\n",
        "            tracker.init(frame, (x1, y1, x2 - x1, y2 - y1))\n",
        "            tracker_active = True\n",
        "\n",
        "    else:\n",
        "        if tracker_active and tracker is not None:\n",
        "            success, tracked_box = tracker.update(frame)\n",
        "            if success:\n",
        "                x, y, w_box, h_box = map(int, tracked_box)\n",
        "                tracked_detection = {\n",
        "                    'box': [x, y, x + w_box, y + h_box],\n",
        "                    'score': None,\n",
        "                    'label': 'tracked'\n",
        "                }\n",
        "                last_detection = tracked_detection\n",
        "            else:\n",
        "                tracker_active = False\n",
        "                tracker = None\n",
        "\n",
        "    detection_to_draw = current_detection or last_detection\n",
        "\n",
        "    if detection_to_draw:\n",
        "        box = detection_to_draw['box']\n",
        "        score = detection_to_draw['score']\n",
        "        label = detection_to_draw.get('label', '')\n",
        "\n",
        "        # draw box\n",
        "        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "        if score is not None:\n",
        "            cv2.putText(frame, f\"{label} {score:.2f}\", (box[0], box[1] - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "        else:\n",
        "            cv2.putText(frame, f\"{label}\", (box[0], box[1] - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "\n",
        "        # Pose estimation\n",
        "        cropped = frame[box[1]:box[3], box[0]:box[2]]\n",
        "        if cropped.size > 0:\n",
        "            pose_results = pose_model.predict(cropped, imgsz=256, conf=0.3, verbose=False)\n",
        "            for kp in pose_results[0].keypoints.xy:\n",
        "                for x, y in kp:\n",
        "                    cx = int(x.item() + box[0])\n",
        "                    cy = int(y.item() + box[1])\n",
        "                    cv2.circle(frame, (cx, cy), 3, (0, 0, 255), -1)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "end_execution_time = time.time()\n",
        "print(\"Время выполнения:\", end_execution_time - start_execution_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный код является оптимизированной версией прошлого решения.\n",
        "\n",
        "В процессе работы видео последовательно считывается покадрово, и на каждом кадре производится **либо первичная детекция** с помощью модели **Grounding DINO**, **либо уточнение** и обновление локализации спортсмена с помощью модели YOLO (или любой другой легковесной модели).\n",
        "\n",
        "**Grounding DINO** запускается **с низкой частотой** и выполняет обнаружение объектов на основе текстовых описаний, при этом обработка происходит на уменьшенной копии кадра для ускорения работы. Найденные координаты масштабируются обратно к исходному размеру изображения.\n",
        "\n",
        "В промежутках **между вызовами DINO**, модель **YOLO** применяется для **дообнаружения** или **уточнения** позиции спортсмена **в ограниченной области** вокруг **предыдущей детекции**.\n",
        "\n",
        "Эта **область расширяется** по горизонтали и вертикали на заданное значение, чтобы обеспечить **более устойчивое обнаружение**.\n",
        "\n",
        "Если YOLO успешно находит человека, его координаты сохраняются и используются **для последующей обработки**.\n",
        "\n",
        "После того как регион спортсмена определён, он обрезается, и на него запускается **модель оценки поз YOLO Pose**, которая определяет **ключевые точки тела**.\n",
        "\n",
        "Эти ключевые точки наносятся на исходный кадр в виде красных точек. Обработанные кадры записываются **в выходное видео**."
      ],
      "metadata": {
        "id": "MoI6BOthG_bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "\n",
        "start_execution_time = time.time()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
        "\n",
        "yolo_model = YOLO(\"yolo11l.pt\")\n",
        "\n",
        "pose_model = YOLO('yolo11m-pose.pt')\n",
        "\n",
        "# Mixed precision if available\n",
        "use_amp = device == 'cuda'\n",
        "if use_amp:\n",
        "    model = model.half()\n",
        "\n",
        "cap = cv2.VideoCapture('/content/0LtLS9wROrk_E_000731_000738.mp4')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "w, h = int(cap.get(3)), int(cap.get(4))\n",
        "out = cv2.VideoWriter('gdino_out_2.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "\n",
        "\n",
        "# optimization parameters\n",
        "GROUNDING_DINO_FPS = 2\n",
        "GROUNDING_DINO_FRAME_SKIP = fps // GROUNDING_DINO_FPS\n",
        "\n",
        "YOLO_FPS = 7\n",
        "YOLO_FRAME_SKIP = fps // YOLO_FPS\n",
        "\n",
        "SCALE_FACTOR = 1/10\n",
        "EXPANSION_RATIO = 0.4\n",
        "\n",
        "\n",
        "prompt = [[\"a gymnast\", \"a gymnast upside down\"]] # Grounding DINO prompts\n",
        "frame_count = 0\n",
        "last_detection = None\n",
        "\n",
        "small_w, small_h = int(w * SCALE_FACTOR), int(h * SCALE_FACTOR)\n",
        "\n",
        "def expand_box(box, img_w, img_h, ratio_hor, ratio_ver):\n",
        "    x1, y1, x2, y2 = box\n",
        "    w_box = x2 - x1\n",
        "    h_box = y2 - y1\n",
        "    pad_x = int(w_box * ratio_hor / 2)\n",
        "    pad_y = int(h_box * ratio_ver / 2)\n",
        "    x1_new = max(0, x1 - pad_x)\n",
        "    y1_new = max(0, y1 - pad_y)\n",
        "    x2_new = min(img_w, x2 + pad_x)\n",
        "    y2_new = min(img_h, y2 + pad_y)\n",
        "    return [x1_new, y1_new, x2_new, y2_new]\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    current_detection = None\n",
        "\n",
        "    if frame_count % GROUNDING_DINO_FRAME_SKIP == 0:\n",
        "        # resizing image\n",
        "        small_frame = cv2.resize(frame, (small_w, small_h))\n",
        "        img = Image.fromarray(cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # mixed precision\n",
        "        if use_amp:\n",
        "            inputs = {k: v.to(device).half() if v.dtype == torch.float32 else v.to(device)\n",
        "                     for k, v in inputs.items()}\n",
        "        else:\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(**inputs)\n",
        "            else:\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "        results = processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs['input_ids'],\n",
        "            box_threshold=0.3,\n",
        "            text_threshold=0.25,\n",
        "            target_sizes=[(small_h, small_w)]\n",
        "        )\n",
        "\n",
        "        boxes = results[0][\"boxes\"]\n",
        "        scores = results[0][\"scores\"]\n",
        "        labels = results[0][\"labels\"]\n",
        "\n",
        "        if len(scores) > 0:\n",
        "            # get index with the highest confidence\n",
        "            max_score_idx = torch.argmax(scores).item()\n",
        "\n",
        "            best_box = boxes[max_score_idx].tolist()\n",
        "            best_score = scores[max_score_idx].item()\n",
        "            best_label = labels[max_score_idx]\n",
        "\n",
        "            # rescaling image\n",
        "            scale_x = w / small_w\n",
        "            scale_y = h / small_h\n",
        "            best_box = [\n",
        "                int(best_box[0] * scale_x),\n",
        "                int(best_box[1] * scale_y),\n",
        "                int(best_box[2] * scale_x),\n",
        "                int(best_box[3] * scale_y)\n",
        "            ]\n",
        "\n",
        "            current_detection = {\n",
        "                'box': best_box,\n",
        "                'score': best_score,\n",
        "                'label': best_label\n",
        "            }\n",
        "            last_detection = current_detection\n",
        "\n",
        "    elif (frame_count % YOLO_FRAME_SKIP == 0) and (frame_count % GROUNDING_DINO_FRAME_SKIP != 0):\n",
        "        area_to_detect = current_detection or last_detection\n",
        "        area_to_detect = area_to_detect[\"box\"]\n",
        "        area_to_detect = expand_box(area_to_detect, w, h, EXPANSION_RATIO, EXPANSION_RATIO + 0.3)\n",
        "\n",
        "        x1, y1, x2, y2 = area_to_detect\n",
        "        cropped_frame = frame[y1:y2, x1:x2]\n",
        "        results = yolo_model(cropped_frame, verbose=False)\n",
        "\n",
        "        detections = results[0].boxes\n",
        "        if detections is not None and detections.xyxy.shape[0] > 0:\n",
        "            boxes = detections.xyxy\n",
        "            scores = detections.conf\n",
        "            classes = detections.cls\n",
        "\n",
        "            person_indices = (classes == 0).nonzero(as_tuple=True)[0]\n",
        "            if len(person_indices) > 0:\n",
        "                # get the best prediction\n",
        "                best_idx = person_indices[scores[person_indices].argmax().item()]\n",
        "                box = boxes[best_idx].tolist()\n",
        "                score = scores[best_idx].item()\n",
        "                label = int(classes[best_idx].item())\n",
        "\n",
        "                current_detection = {\n",
        "                    'box': [int(box[0] + x1), int(box[1] + y1), int(box[2] + x1), int(box[3] + y1)],\n",
        "                    'score': score,\n",
        "                    'label': label\n",
        "                }\n",
        "                last_detection = current_detection\n",
        "\n",
        "    detection_to_draw = current_detection or last_detection\n",
        "\n",
        "    if detection_to_draw:\n",
        "        box = detection_to_draw['box']\n",
        "        score = detection_to_draw['score']\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"{prompt[0]} {score:.2f}\", (box[0], box[1]-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "        # Crop the person region for pose estimation\n",
        "        cropped = frame[box[1]:box[3], box[0]:box[2]]\n",
        "        if cropped.size > 0:\n",
        "            pose_results = pose_model.predict(cropped, imgsz=256, conf=0.3, verbose=False)\n",
        "\n",
        "            # Draw keypoints on the original frame (rescale points to original image)\n",
        "            for kp in pose_results[0].keypoints.xy:\n",
        "                for x, y in kp:\n",
        "                    cx = int(x.item() + box[0])\n",
        "                    cy = int(y.item() + box[1])\n",
        "                    cv2.circle(frame, (cx, cy), 3, (0, 0, 255), -1)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "end_execution_time = time.time()\n",
        "print(\"Время выполнения:\", end_execution_time - start_execution_time)"
      ],
      "metadata": {
        "id": "-jsiG3Sg05n6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
