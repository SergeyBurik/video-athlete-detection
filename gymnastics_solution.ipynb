{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Below are **two** solutions to the problem.\n",
        "\n",
        "**The first** uses the **Grounding DINO** model, which finds the athlete based on **a textual description**. The model works with a minimal number of false detections and consistently delivers **accurate results**. However, each invocation of this model takes longer than lighter alternatives like YOLO.\n",
        "\n",
        "**The second** solution is an **optimized version** of the first one. **It minimizes** the number of Grounding DINO model **calls**. This model is invoked only twice per second. The bounding boxes from this model serve as anchors for **a faster model** like YOLO (or any similar model).\n",
        "\n"
      ],
      "metadata": {
        "id": "ZgmrzaDyH5l5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code implements a system for **automatic detection, tracking, and pose estimation** of an athlete in a video.\n",
        "\n",
        "It uses the **Grounding DINO model** for text-based detection at a frequency of 5 times per second. Then the **CSRT tracker** is activated to maintain tracking of the athlete **between detections**.\n",
        "\n",
        "To increase speed, detection is performed on **a downscaled copy** of the frame, and the coordinates are scaled back accordingly.\n",
        "\n",
        "If the object is **successfully detected** or tracked, the corresponding **area is cropped**, and the YOLO model is then applied for **pose estimation**.\n",
        "\n",
        "The resulting keypoints are displayed on the original frame as **red dots**.\n",
        "\n",
        "The final video is saved to an **output file**. The code also supports GPU execution with mixed precision to speed up computation."
      ],
      "metadata": {
        "id": "raCMpDP1Eoy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install deep-sort-realtime\n",
        "!pip install openmim\n",
        "!mim install mmdet\n",
        "!mim install mmpose\n",
        "\n",
        "# Grounding Dino\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "!cd GroundingDINO\n",
        "!pip install --upgrade pip setuptools wheel cython\n",
        "!pip install -r requirements.txt\n",
        "!pip install pycocotools supervision yapf addict\n",
        "!pip install -e . --no-build-isolation\n",
        "\n"
      ],
      "metadata": {
        "id": "aivqb7jI0p-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LK6Zjaf0lUn"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "\n",
        "start_execution_time = time.time()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
        "\n",
        "pose_model = YOLO('yolo11m-pose.pt')\n",
        "\n",
        "# Mixed precision\n",
        "use_amp = device == 'cuda'\n",
        "if use_amp:\n",
        "    model = model.half()\n",
        "\n",
        "cap = cv2.VideoCapture('input.mp4')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "w, h = int(cap.get(3)), int(cap.get(4))\n",
        "out = cv2.VideoWriter('gdino_out_1.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "\n",
        "# Parameters\n",
        "DETECTION_FPS = 5  # times per second\n",
        "FRAME_SKIP = int(fps / DETECTION_FPS)\n",
        "SCALE_FACTOR = 1/10\n",
        "prompt = [[\"a gymnast\", \"a gymnast upside down\"]]\n",
        "\n",
        "frame_count = 0\n",
        "last_detection = None\n",
        "tracker = None\n",
        "tracker_active = False\n",
        "\n",
        "small_w, small_h = int(w * SCALE_FACTOR), int(h * SCALE_FACTOR)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    current_detection = None\n",
        "\n",
        "    if frame_count % FRAME_SKIP == 0:\n",
        "        small_frame = cv2.resize(frame, (small_w, small_h))\n",
        "        img = Image.fromarray(cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB))\n",
        "        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n",
        "\n",
        "        if use_amp:\n",
        "            inputs = {k: v.to(device).half() if v.dtype == torch.float32 else v.to(device)\n",
        "                      for k, v in inputs.items()}\n",
        "        else:\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(**inputs)\n",
        "            else:\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "        results = processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs['input_ids'],\n",
        "            box_threshold=0.3,\n",
        "            text_threshold=0.25,\n",
        "            target_sizes=[(small_h, small_w)]\n",
        "        )\n",
        "\n",
        "        boxes = results[0][\"boxes\"]\n",
        "        scores = results[0][\"scores\"]\n",
        "        labels = results[0][\"labels\"]\n",
        "\n",
        "        if len(scores) > 0:\n",
        "            max_score_idx = torch.argmax(scores).item()\n",
        "            best_box = boxes[max_score_idx].tolist()\n",
        "            best_score = scores[max_score_idx].item()\n",
        "            best_label = labels[max_score_idx]\n",
        "\n",
        "            scale_x = w / small_w\n",
        "            scale_y = h / small_h\n",
        "            best_box = [\n",
        "                int(best_box[0] * scale_x),\n",
        "                int(best_box[1] * scale_y),\n",
        "                int(best_box[2] * scale_x),\n",
        "                int(best_box[3] * scale_y)\n",
        "            ]\n",
        "\n",
        "            current_detection = {\n",
        "                'box': best_box,\n",
        "                'score': best_score,\n",
        "                'label': best_label\n",
        "            }\n",
        "            last_detection = current_detection\n",
        "\n",
        "            tracker = cv2.TrackerCSRT_create()\n",
        "            x1, y1, x2, y2 = best_box\n",
        "            tracker.init(frame, (x1, y1, x2 - x1, y2 - y1))\n",
        "            tracker_active = True\n",
        "\n",
        "    else:\n",
        "        if tracker_active and tracker is not None:\n",
        "            success, tracked_box = tracker.update(frame)\n",
        "            if success:\n",
        "                x, y, w_box, h_box = map(int, tracked_box)\n",
        "                tracked_detection = {\n",
        "                    'box': [x, y, x + w_box, y + h_box],\n",
        "                    'score': None,\n",
        "                    'label': 'tracked'\n",
        "                }\n",
        "                last_detection = tracked_detection\n",
        "            else:\n",
        "                tracker_active = False\n",
        "                tracker = None\n",
        "\n",
        "    detection_to_draw = current_detection or last_detection\n",
        "\n",
        "    if detection_to_draw:\n",
        "        box = detection_to_draw['box']\n",
        "        score = detection_to_draw['score']\n",
        "        label = detection_to_draw.get('label', '')\n",
        "\n",
        "        # draw box\n",
        "        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "        if score is not None:\n",
        "            cv2.putText(frame, f\"{label} {score:.2f}\", (box[0], box[1] - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "        else:\n",
        "            cv2.putText(frame, f\"{label}\", (box[0], box[1] - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
        "\n",
        "        # Pose estimation\n",
        "        cropped = frame[box[1]:box[3], box[0]:box[2]]\n",
        "        if cropped.size > 0:\n",
        "            pose_results = pose_model.predict(cropped, imgsz=256, conf=0.3, verbose=False)\n",
        "            for kp in pose_results[0].keypoints.xy:\n",
        "                for x, y in kp:\n",
        "                    cx = int(x.item() + box[0])\n",
        "                    cy = int(y.item() + box[1])\n",
        "                    cv2.circle(frame, (cx, cy), 3, (0, 0, 255), -1)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "end_execution_time = time.time()\n",
        "print(\"Execution time:\", end_execution_time - start_execution_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is an optimized version of the previous solution.\n",
        "\n",
        "During execution, the video is read frame by frame, and on each frame, **either a primary detection** is performed using the **Grounding DINO** model, **or a refinement** and update of the athlete’s localization is done using the YOLO model (or any other lightweight model).\n",
        "\n",
        "**Grounding DINO** is run **at a low frequency** and performs object detection based on textual descriptions. The processing is done on a downscaled copy of the frame to speed up execution. The detected coordinates are then scaled back to the original image size.\n",
        "\n",
        "In the intervals **between DINO calls**, the **YOLO** model is applied for **additional detection** or **refinement** of the athlete’s position **within a limited area** around the **previous detection**.\n",
        "\n",
        "This **area is expanded** horizontally and vertically by a specified value to ensure **more stable detection**.\n",
        "\n",
        "If YOLO successfully detects a person, their coordinates are saved and used **for further processing**.\n",
        "\n",
        "Once the athlete’s region is identified, it is cropped, and the **YOLO Pose estimation model** is run on it to determine the **body keypoints**.\n",
        "\n",
        "These keypoints are drawn on the original frame as red dots. The processed frames are saved **to the output video**.\n"
      ],
      "metadata": {
        "id": "MoI6BOthG_bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "\n",
        "start_execution_time = time.time()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
        "\n",
        "yolo_model = YOLO(\"yolo11l.pt\")\n",
        "\n",
        "pose_model = YOLO('yolo11m-pose.pt')\n",
        "\n",
        "# Mixed precision if available\n",
        "use_amp = device == 'cuda'\n",
        "if use_amp:\n",
        "    model = model.half()\n",
        "\n",
        "cap = cv2.VideoCapture('input.mp4')\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "w, h = int(cap.get(3)), int(cap.get(4))\n",
        "out = cv2.VideoWriter('gdino_out_2.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "\n",
        "\n",
        "# optimization parameters\n",
        "GROUNDING_DINO_FPS = 2\n",
        "GROUNDING_DINO_FRAME_SKIP = fps // GROUNDING_DINO_FPS\n",
        "\n",
        "YOLO_FPS = 7\n",
        "YOLO_FRAME_SKIP = fps // YOLO_FPS\n",
        "\n",
        "SCALE_FACTOR = 1/10\n",
        "EXPANSION_RATIO = 0.4\n",
        "\n",
        "\n",
        "prompt = [[\"a gymnast\", \"a gymnast upside down\"]] # Grounding DINO prompts\n",
        "frame_count = 0\n",
        "last_detection = None\n",
        "\n",
        "small_w, small_h = int(w * SCALE_FACTOR), int(h * SCALE_FACTOR)\n",
        "\n",
        "def expand_box(box, img_w, img_h, ratio_hor, ratio_ver):\n",
        "    x1, y1, x2, y2 = box\n",
        "    w_box = x2 - x1\n",
        "    h_box = y2 - y1\n",
        "    pad_x = int(w_box * ratio_hor / 2)\n",
        "    pad_y = int(h_box * ratio_ver / 2)\n",
        "    x1_new = max(0, x1 - pad_x)\n",
        "    y1_new = max(0, y1 - pad_y)\n",
        "    x2_new = min(img_w, x2 + pad_x)\n",
        "    y2_new = min(img_h, y2 + pad_y)\n",
        "    return [x1_new, y1_new, x2_new, y2_new]\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    current_detection = None\n",
        "\n",
        "    if frame_count % GROUNDING_DINO_FRAME_SKIP == 0:\n",
        "        # resizing image\n",
        "        small_frame = cv2.resize(frame, (small_w, small_h))\n",
        "        img = Image.fromarray(cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # mixed precision\n",
        "        if use_amp:\n",
        "            inputs = {k: v.to(device).half() if v.dtype == torch.float32 else v.to(device)\n",
        "                     for k, v in inputs.items()}\n",
        "        else:\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(**inputs)\n",
        "            else:\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "        results = processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs['input_ids'],\n",
        "            box_threshold=0.3,\n",
        "            text_threshold=0.25,\n",
        "            target_sizes=[(small_h, small_w)]\n",
        "        )\n",
        "\n",
        "        boxes = results[0][\"boxes\"]\n",
        "        scores = results[0][\"scores\"]\n",
        "        labels = results[0][\"labels\"]\n",
        "\n",
        "        if len(scores) > 0:\n",
        "            # get index with the highest confidence\n",
        "            max_score_idx = torch.argmax(scores).item()\n",
        "\n",
        "            best_box = boxes[max_score_idx].tolist()\n",
        "            best_score = scores[max_score_idx].item()\n",
        "            best_label = labels[max_score_idx]\n",
        "\n",
        "            # rescaling image\n",
        "            scale_x = w / small_w\n",
        "            scale_y = h / small_h\n",
        "            best_box = [\n",
        "                int(best_box[0] * scale_x),\n",
        "                int(best_box[1] * scale_y),\n",
        "                int(best_box[2] * scale_x),\n",
        "                int(best_box[3] * scale_y)\n",
        "            ]\n",
        "\n",
        "            current_detection = {\n",
        "                'box': best_box,\n",
        "                'score': best_score,\n",
        "                'label': best_label\n",
        "            }\n",
        "            last_detection = current_detection\n",
        "\n",
        "    elif (frame_count % YOLO_FRAME_SKIP == 0) and (frame_count % GROUNDING_DINO_FRAME_SKIP != 0):\n",
        "        area_to_detect = current_detection or last_detection\n",
        "        area_to_detect = area_to_detect[\"box\"]\n",
        "        area_to_detect = expand_box(area_to_detect, w, h, EXPANSION_RATIO, EXPANSION_RATIO + 0.3)\n",
        "\n",
        "        x1, y1, x2, y2 = area_to_detect\n",
        "        cropped_frame = frame[y1:y2, x1:x2]\n",
        "        results = yolo_model(cropped_frame, verbose=False)\n",
        "\n",
        "        detections = results[0].boxes\n",
        "        if detections is not None and detections.xyxy.shape[0] > 0:\n",
        "            boxes = detections.xyxy\n",
        "            scores = detections.conf\n",
        "            classes = detections.cls\n",
        "\n",
        "            person_indices = (classes == 0).nonzero(as_tuple=True)[0]\n",
        "            if len(person_indices) > 0:\n",
        "                # get the best prediction\n",
        "                best_idx = person_indices[scores[person_indices].argmax().item()]\n",
        "                box = boxes[best_idx].tolist()\n",
        "                score = scores[best_idx].item()\n",
        "                label = int(classes[best_idx].item())\n",
        "\n",
        "                current_detection = {\n",
        "                    'box': [int(box[0] + x1), int(box[1] + y1), int(box[2] + x1), int(box[3] + y1)],\n",
        "                    'score': score,\n",
        "                    'label': label\n",
        "                }\n",
        "                last_detection = current_detection\n",
        "\n",
        "    detection_to_draw = current_detection or last_detection\n",
        "\n",
        "    if detection_to_draw:\n",
        "        box = detection_to_draw['box']\n",
        "        score = detection_to_draw['score']\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, f\"{prompt[0]} {score:.2f}\", (box[0], box[1]-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "        # Crop the person region for pose estimation\n",
        "        cropped = frame[box[1]:box[3], box[0]:box[2]]\n",
        "        if cropped.size > 0:\n",
        "            pose_results = pose_model.predict(cropped, imgsz=256, conf=0.3, verbose=False)\n",
        "\n",
        "            # Draw keypoints on the original frame (rescale points to original image)\n",
        "            for kp in pose_results[0].keypoints.xy:\n",
        "                for x, y in kp:\n",
        "                    cx = int(x.item() + box[0])\n",
        "                    cy = int(y.item() + box[1])\n",
        "                    cv2.circle(frame, (cx, cy), 3, (0, 0, 255), -1)\n",
        "\n",
        "    out.write(frame)\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "end_execution_time = time.time()\n",
        "print(\"Execution time:\", end_execution_time - start_execution_time)"
      ],
      "metadata": {
        "id": "-jsiG3Sg05n6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
